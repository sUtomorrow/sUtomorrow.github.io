<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sutomorrow.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="线性模型 基于均方误差最小化来进行求解的方法称为最小二乘法 用最小二乘法来优化线性回归 线性回归的目标是学习函数\(f(X) &#x3D; Xw\)，使得\(f(X) \approx Y\)，其中\(X&#x3D;\begin{bmatrix}x_1 &amp; 1\\ x_2 &amp; 1\\ \vdots &amp; \vdots \\ x_m &amp; 1 \end{bmatrix}\)，\(Y&#x3D;\begi">
<meta property="og:type" content="article">
<meta property="og:title" content="《机器学习》读书笔记">
<meta property="og:url" content="https://sutomorrow.github.io/2020/04/17/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Tennen&#39;s Blog">
<meta property="og:description" content="线性模型 基于均方误差最小化来进行求解的方法称为最小二乘法 用最小二乘法来优化线性回归 线性回归的目标是学习函数\(f(X) &#x3D; Xw\)，使得\(f(X) \approx Y\)，其中\(X&#x3D;\begin{bmatrix}x_1 &amp; 1\\ x_2 &amp; 1\\ \vdots &amp; \vdots \\ x_m &amp; 1 \end{bmatrix}\)，\(Y&#x3D;\begi">
<meta property="og:locale">
<meta property="article:published_time" content="2020-04-17T09:27:30.000Z">
<meta property="article:modified_time" content="2023-10-12T11:15:02.029Z">
<meta property="article:author" content="Tennen">
<meta property="article:tag" content="杂项">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="读书笔记">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://sutomorrow.github.io/2020/04/17/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>《机器学习》读书笔记 | Tennen's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tennen's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://sutomorrow.github.io/2020/04/17/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Tennen">
      <meta itemprop="description" content="record something">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tennen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《机器学习》读书笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-17 17:27:30" itemprop="dateCreated datePublished" datetime="2020-04-17T17:27:30+08:00">2020-04-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-10-12 19:15:02" itemprop="dateModified" datetime="2023-10-12T19:15:02+08:00">2023-10-12</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="线性模型">线性模型</h1>
<h2 id="基于均方误差最小化来进行求解的方法称为最小二乘法">基于均方误差最小化来进行求解的方法称为最小二乘法</h2>
<h2 id="用最小二乘法来优化线性回归">用最小二乘法来优化线性回归</h2>
<p>线性回归的目标是学习函数<span class="math inline">\(f(X) =
Xw\)</span>，使得<span class="math inline">\(f(X) \approx
Y\)</span>，其中<span class="math inline">\(X=\begin{bmatrix}x_1 &amp;
1\\ x_2 &amp; 1\\ \vdots &amp; \vdots \\ x_m &amp; 1
\end{bmatrix}\)</span>，<span class="math inline">\(Y=\begin{bmatrix}y_1
\\ y_2 \\ \vdots \\ y_m\end{bmatrix}\)</span>，<span class="math inline">\(w \in R^{(n+1) \times 1}\)</span>，<span class="math inline">\(n\)</span>是数据特征维数。</p>
<p>如果用均方误差作为损失函数<span class="math inline">\(L = (Xw-Y)^T
(Xw-Y)\)</span>，那么问题可以描述为</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\arg \min_{w} (Xw-Y)^T (Xw-Y)\\
\end{aligned}
\]</span> 直接求导： <span class="math display">\[
\frac{\partial L}{\partial w} = \frac{\partial (w^T X^T X w - Y^TXw -XwY
- Y^TY)}{\partial w} = 2X^T(Xw-Y)\\
\]</span> 令： <span class="math display">\[
\begin{aligned}
&amp;\frac{\partial L}{\partial w} = 0\\
&amp;\Rightarrow w = (X^TX)^{-1}X^TY
\end{aligned}
\]</span> 即得到使用均方误差的线性回归问题的解。</p>
<h2 id="对数几率回归逻辑回归-logistic-regression">对数几率回归（逻辑回归，
Logistic Regression）</h2>
<p>对数几率回归即用线性回归去拟合对数几率<span class="math inline">\(ln\frac{y}{1-y} = w^T x + b\)</span></p>
<p>对数几率回归也等价于<span class="math inline">\(y=\frac{1}{1+e^{-z}},
z = w^T x + b\)</span></p>
<p>若将<span class="math inline">\(y\)</span>视为后验概率，则<span class="math inline">\(ln \frac{P(y=1|x)}{P(y=0|x)} = z, z=w^Tx +
b\)</span></p>
<p>显然有<span class="math inline">\(P(y=1|x) = \frac{e^z}{1 + e^z},
P(y=0|x)=\frac{1}{1 + e^z}\)</span></p>
<p>使用极大似然法求解对数几率回归：<span class="math inline">\(\mathop{\arg\max}\limits_{w,
b}\prod\limits_{i=1}^m P(y=y_i| x_i) \Rightarrow
\mathop{\arg\max}\limits_{w, b}\sum\limits_{i=1}^m
ln(P(y=y_i|x_i))\)</span></p>
<p>令<span class="math inline">\(\beta = \begin{bmatrix}w ^ T&amp;
b\end{bmatrix} ^ T \in R^{n+1},
Y=\begin{bmatrix}y_1&amp;y_2&amp;\dots&amp;y_m\end{bmatrix} ^ T,
X=\begin{bmatrix}
x_1&amp;x_2&amp;\dots&amp;x_m\\1&amp;1&amp;\dots&amp;1\end{bmatrix},x_i
\in R^n, X \in R^{(n+1) \times m}\)</span>，其中<span class="math inline">\(m\)</span>是数据量。</p>
<p>使用极大似然法求解对数几率回归可以重写为： <span class="math display">\[
\begin{aligned}
&amp;\mathop{\arg\max}\limits_{\beta} l(Z)\\
&amp;Z = X^T \beta\\
&amp;l(Z) = Y^Tln\frac{e^Z}{\mathbf{1} + e^Z} +
(\mathbf{1}-Y)^Tln\frac{\mathbf{1}}{\mathbf{1}+e^Z}\\
&amp;=Y^TZ - ln(\mathbf{1}+e^Z)
\end{aligned}
\]</span></p>
<p>使用牛顿法，第<span class="math inline">\(t\)</span>次更新为<span class="math inline">\(\beta^{t+1} \leftarrow \beta ^ t -
(\triangledown_2l)^{-1}\frac{\partial l}{\partial \beta}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
dl &amp;= Y^TdZ - \mathbf{1}^T\frac{e^Z}{\mathbf{1}+e^Z} \odot dZ\\
&amp;=Y^TdZ -\mathbf{1}^T \hat{P}_1 \odot dZ, \hat{P} = \begin{bmatrix}
P(y=1|x_1) &amp; P(y=1|x_2)&amp; \dots &amp; P(y=1|x_m)\end{bmatrix}^T\\
&amp;=Y^TX^Td\beta - \mathbf{1}^T \hat{P}_1 \odot (X^Td\beta)\\
&amp;=Y^TX^Td\beta - (\mathbf{1} \odot \hat{P}_1)^TX^Td\beta\\
&amp;=(Y^T-\hat{P}_1^T)X^Td\beta
\end{aligned}
\]</span></p>
<p>所以<span class="math inline">\(\frac{\partial l}{\partial \beta} =
X(Y-\hat{P}_1)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
    d(\frac{\partial l}{\partial \beta}) &amp;= d(X(Y-\hat{P}_1))\\
    &amp;=Xd\hat{P}_1\\
    &amp;=Xd\frac{e^Z}{\mathbf{1}+e^Z}\\
    &amp;=X(\frac{1}{1+e^Z}\odot\frac{e^Z}{1+e^Z} \odot dZ)\\
    &amp;=X(\hat{P}_0 \odot \hat{P}_1 \odot (X^Td\beta))\\
    &amp;=X diag(\hat{P}_0) diag(\hat{P}_1) X^Td\beta,\ diag(\hat{P}_0)
= \begin{bmatrix}
        P(y=0|x_1)&amp;\cdots&amp;0\\
        \vdots&amp;\ddots&amp;\vdots\\
        0&amp;\cdots&amp;P(y=0|x_m)
    \end{bmatrix}
\end{aligned}
\]</span></p>
<p>所以<span class="math inline">\(\frac{\partial^2 l}{\partial \beta
\partial \beta^T} = \frac{\partial^2 l}{\partial \beta^T \partial \beta}
= Xdiag(\hat{P}_1) diag(\hat{P}_0)X^T\)</span></p>
<p>即如果用牛顿法来求解极大似然对数几率回归，第<span class="math inline">\(t\)</span>次更新为</p>
<p><span class="math display">\[
\beta^{t+1} \leftarrow \beta ^ t - (Xdiag(\hat{P}_1)
diag(\hat{P}_0)X^T)^{-1} X(Y-\hat{P}_1)
\]</span></p>
<h2 id="线性判别分析">线性判别分析</h2>
<p>线性判别分析也叫“Fisher判别分析”，其思想非常朴素：试图找到一个低维空间（直线），可以表示为<span class="math inline">\(y=wx\)</span>(这个表达式中的<span class="math inline">\(y\)</span>表示<span class="math inline">\(x\)</span>投影到这个空间（直线）后和原点的距离)，使得当样本投影到该直线时不同类别样本的距离尽可能远，而相同类别的数据尽可能接近，预测时按照样本的投影位置对其进行分类。</p>
<p>以两类数据<span class="math inline">\(x_1, x_2\)</span>为例，设<span class="math inline">\(\mu_1,\mu_2,\Sigma_1,\Sigma_2\)</span>分别表示两类数据的均值和方差，则投影之后的均值和方差为<span class="math inline">\(w\mu_1,w\mu_2,w^T\Sigma_1w,w^T\Sigma_2w\)</span>，因为直线是一维空间，所以这些均为实数，投影之后的类内距离可以用方差来衡量，这里使用<span class="math inline">\(w^T\Sigma_1w +
w^T\Sigma_2w\)</span>来度量投影之后的类内距离，而类间距离可以写成<span class="math inline">\(||w\mu_2 -
w\mu_1||_2^2\)</span>，同时考虑两种距离，给出希望最大化的目标函数如下。
<span class="math display">\[
\begin{aligned}
J &amp;= \frac{||w^T\mu_2 - w^T\mu_1||_2^2}{w^T\Sigma_1w +
w^T\Sigma_2w}\\
&amp;= \frac{w^T(\mu_2 - \mu_1)(\mu_2 - \mu_1)^Tw}{w^T(\Sigma_1 +
\Sigma_2)w}
\end{aligned}
\]</span> 定义类内散度矩阵<span class="math inline">\(S_w = \Sigma_1 +
\Sigma_2\)</span>，类间散度矩阵<span class="math inline">\(S_b = (\mu_2
- \mu_1)(\mu_2 - \mu_1)^T\)</span>，上面的优化目标可以简写为如下。 <span class="math display">\[
\begin{aligned}
    J = \frac{w^TS_bw}{w^TS_ww}
\end{aligned}
\]</span> 这个优化目标又称为<span class="math inline">\(S_b\)</span>和<span class="math inline">\(S_w\)</span>的广义瑞利商，注意到分子分母中都有<span class="math inline">\(w\)</span>的二次项，因此和<span class="math inline">\(w\)</span>大小无关，只和w方向有关，所以优化问题可以写成下式。
<span class="math display">\[
\begin{aligned}
\min_w&amp; \quad-w^TS_bw\\
s.t.&amp; \quad w^TS_ww = 1
\end{aligned}
\]</span> 用拉格朗日乘子法进行优化，求解<span class="math inline">\(S_bw
= \lambda S_ww\)</span>，因<span class="math inline">\(S_bw\)</span>方向和<span class="math inline">\(\mu_2 - \mu_1\)</span>相同，因此令<span class="math inline">\(S_bw = \lambda(\mu_2 -
\mu_1)\)</span>，代入求解，可以得到<span class="math inline">\(w =
S_w^{-1}(\mu_2 - \mu_1)\)</span>。</p>
<p>将数据进行降维，使得类内散度最小，类间散度最大，本质上是有监督的降维。</p>
<h2 id="多分类">多分类</h2>
<p>可以将多分类问题拆解为二分类问题，拆解策略有三种：一对一（OvO）、一对其余（OvR）、多对多（MvM）</p>
<p>在MvM中，最常用的是纠错输出码（Error Correcting Output
Codes，ECOC）,有<span class="math inline">\(C_1C_2C_3...C_n\)</span>共<span class="math inline">\(n\)</span>个类别，每个样本属于其中的一种，训练m个二分类器<span class="math inline">\(f_1, f_2, ...,
f_m\)</span>，每个分类器将一些类作为正类，另一些类作为负类，这样对于某个类别的样本，理想情况是<span class="math inline">\(m\)</span>个分类器对其进行预测的输出组成的0,1串，构成一种长度为<span class="math inline">\(m\)</span>的固定的类别组合串，<span class="math inline">\(n\)</span>个类就有<span class="math inline">\(n\)</span>种组合，但在预测时，对一个样本预测得到的输出串，可能不在<span class="math inline">\(n\)</span>个类的<span class="math inline">\(n\)</span>种组合中，这时，计算预测输出串和每个类别组合串的距离（海明距离或者欧式距离），将样本判定为距离最小的那个类别组合串对应的类别。</p>
<h2 id="类别不平衡">类别不平衡</h2>
<p>解决办法主要有三种： -
再缩放（再平衡），根据样本数量移动判定阈值或者缩放预测概率。 -
欠采样，将样本量过多的类别进行采样，减少该类别的样本数量，再拿去训练，但是这个方法容易丢失数据中的信息，最好是分成多个模型，每个模型使用该类别的一部分数据。
-
过采样，将样本量过少的类别样本进行重复，然后训练，但是这个方法容易严重过拟合，一个办法是用两个该类别样本进行插值，生成新的该类别样本。</p>
<h1 id="决策树">决策树</h1>
<h2 id="信息熵">信息熵</h2>
<p>样本集合<span class="math inline">\(D\)</span>中第<span class="math inline">\(k\)</span>类样本所占比例为<span class="math inline">\(p_k\)</span>，则信息熵定义为<span class="math inline">\(Ent(D) = -\sum\limits_{k=1}^C
p_k\log_2p_k\)</span>，其中<span class="math inline">\(C\)</span>为类别个数。</p>
<h2 id="信息熵增益">信息熵增益</h2>
<p>假设离散属性<span class="math inline">\(a\)</span>有<span class="math inline">\(v\)</span>个取值：<span class="math inline">\(a_1,
a_2, ..., a_v\)</span>，可以将当前数据集合分成<span class="math inline">\(V\)</span>个子集：<span class="math inline">\(D_1,
D_2, ..., D^V\)</span>，那么信息熵增益定义为<span class="math inline">\(Gain(D,a)=Ent(D)-\sum\limits_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)\)</span></p>
<p>决策树构造过程，即每次选择一个信息熵增益最大的属性<span class="math inline">\(a\)</span>，将数据划分为<span class="math inline">\(V\)</span>个子集，然后在每个子集中进行同样的操作（不能选择已经使用过的属性），若无属性可用或者当前子集类别相同，则当前子集标记为类别数量最多的类别，停止继续划分。</p>
<h2 id="增益率">增益率</h2>
<p>信息熵增益的定义导致其对数量较多的<span class="math inline">\(D^v\)</span>更加敏感，因此又提出了增益率的概念：<span class="math inline">\(Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}\)</span>，其中，<span class="math inline">\(IV(a)=-\sum\limits_{v=1}^V \frac{|D^v|}{|D|}
\log_2\frac{|D^v|}{|D|}\)</span>，称为属性<span class="math inline">\(a\)</span>的固有值。</p>
<h2 id="基尼指数">基尼指数</h2>
<p>基尼值定义为<span class="math inline">\(Gini(D) =
\sum\limits_{k=1}^C\sum\limits_{k&#39; \ne k}p_k p_{k&#39;} =
1-\sum\limits_{k=1}^Cp_k^2\)</span>，其反映了在<span class="math inline">\(D\)</span>中随机抽取两个样本，属于同一类别的概率。
和信息熵增益类似，定义基尼指数为<span class="math inline">\(Gini_index(D, a) =
\sum\limits_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)\)</span>，构造决策树时的最优划分属性选择也可以使用能够使基尼指数最小的属性。</p>
<h1 id="决策树的剪枝">决策树的剪枝</h1>
<h2 id="预剪枝">预剪枝</h2>
<p>在生成决策树的过程中，利用验证集验证当前划分是否能提升泛化效果，如果不能，则停止继续划分，直接将当前样本集作为决策树叶节点，并指定数量最多的类别做为叶节点类别。预剪枝训练时间短，但是由于其是贪心策略，容易引起欠拟合。</p>
<h2 id="后剪枝">后剪枝</h2>
<p>在决策树生成完成之后，自底向上的对非叶节点进行考察，观察将该节点直接指定为叶节点是否能提升验证集泛化效果，如果能则将此节点的子树一起合并为一个叶节点。后剪枝的训练时间比预剪枝长很多，但是其欠拟合风险小，而且泛化性能往往优于预剪枝。</p>
<h1 id="包含连续值的决策树">包含连续值的决策树</h1>
<p>如果连续属性<span class="math inline">\(a\)</span>在<span class="math inline">\(D\)</span>中出现<span class="math inline">\(n\)</span>个取值，则将其从小到大排序为<span class="math inline">\(\begin{bmatrix}a_1, a_2, ...
a_n\end{bmatrix}\)</span>，这样产生<span class="math inline">\(n-1\)</span>个离散值<span class="math inline">\(T_a = \{\frac{a_i + a_{i+1}}{2}|1 \le i \le
n-1\}\)</span> 则<span class="math inline">\(Gain(D,a)= \max\limits_{t
\in T_a}Gain(D, a, t)\)</span>，其中<span class="math inline">\(Gain(D,
a, t)\)</span>表示将<span class="math inline">\(a\)</span>属性使用<span class="math inline">\(t\)</span>划分为两部分，这样，连续值的决策树构造和离散值类似了，不过连续值属性在构造决策树的过程中可以使用多次。</p>
<h1 id="属性缺失的处理">属性缺失的处理</h1>
<p>令<span class="math inline">\(\tilde{D}\)</span>是所有没有缺失属性a的样本集合，对于样本<span class="math inline">\(x\)</span>，有样本权重<span class="math inline">\(w_x\)</span>，定义如下参数。 <span class="math display">\[
\rho = \frac{\sum_{x\in \tilde{D}} w_x}{\sum_{x\in D}w_x}\\
\tilde{p}_k = \frac{\sum_{x\in \tilde{D}_k w_x}}{\sum_{x\in
\tilde{D}}w_x}, (1\le k \le C)\\
\tilde{r}_v = \frac{\sum_{x\in \tilde{D}^v}w_x}{\sum_{x \in \tilde{D}}
w_x}, (1 \le v \le V)
\]</span> 显然，<span class="math inline">\(\rho\)</span>表示属性无缺失样本所占比例，<span class="math inline">\(\tilde{p}_k\)</span>表示属性无缺失样本中第<span class="math inline">\(k\)</span>类所占比例，<span class="math inline">\(\tilde{r}_v\)</span>表示属性无缺失样本中在属性<span class="math inline">\(a\)</span>上取值<span class="math inline">\(a^v\)</span>的样本比例。</p>
<p>由此推广信息增益为： <span class="math display">\[
\begin{aligned}
Gain(D, a) &amp;= \rho \times Gain(\tilde{D}, a)\\
&amp;=\rho \times (Ent(\tilde{D}) - \sum\limits_{v=1}^V \tilde{r}_v
Ent(\tilde{D}^v))
\end{aligned}
\]</span> 其中： <span class="math display">\[
Ent(\tilde{D}) = -\sum\limits_{k=1}^C \tilde{p}_k log_2 \tilde{p}_k
\]</span> 这样解决了最优划分的属性选择问题，在构造子树时，如果样本<span class="math inline">\(x\)</span>在属性<span class="math inline">\(a\)</span>上的取值已知，那么<span class="math inline">\(x\)</span>划分到相应子节点，且权重保持为<span class="math inline">\(w_x\)</span>，如果属性<span class="math inline">\(a\)</span>未知，则将<span class="math inline">\(s\)</span>划分入所有的子节点，且权重调整为<span class="math inline">\(\tilde{r}_v w_x\)</span>。</p>
<h2 id="多变量决策树">多变量决策树</h2>
<p>叶节点不再针对某个属性，而是针对属性的线性组合进行划分。</p>
<h1 id="神经网络">神经网络</h1>
<h2 id="感知机">感知机</h2>
<p>两层神经元，输入层（没有权重，直接前馈数据）和输出层，输出层是M-P神经元（阈值逻辑单元），感知机只能拟合线性可分的数据，否则其学习过程将变得震荡，难以收敛。</p>
<h2 id="bp算法">BP算法</h2>
<p>对于<span class="math inline">\(l\)</span>层神经网络,输入<span class="math inline">\(x \in R^n\)</span>，标签<span class="math inline">\(y \in R^c\)</span>，第<span class="math inline">\(i\)</span>层权重表示为<span class="math inline">\(w_i \in R^{O_i \times I_i}, I_1 = n，O_l =
c\)</span>，第<span class="math inline">\(i\)</span>层偏移表示为<span class="math inline">\(b_i \in R^{O_i}\)</span>，第<span class="math inline">\(i\)</span>层激活函数表示为<span class="math inline">\(\sigma_i\)</span>，这一般是个逐元素函数，第<span class="math inline">\(i\)</span>层输入即第<span class="math inline">\(i-1\)</span>层的输出，表示为<span class="math inline">\(l_{i-1}\)</span>，其中<span class="math inline">\(l_0 = x, z_i = w_i l_{i-1} + b_i, l_i =
\sigma_i(z_i)\)</span></p>
<p>loss函数记为<span class="math inline">\(E(l_l,
y)\)</span>，BP算法每次更新<span class="math inline">\(w_i = w_i - \eta
\frac{\partial E}{\partial w_i}\)</span>， <span class="math inline">\(b_i = b_i - \eta \frac{\partial E}{\partial
b_i}\)</span>，即让参数像梯度最小的方向前进。</p>
<p>首先定义<span class="math inline">\(E\)</span>对<span class="math inline">\(l_l\)</span>的偏导为<span class="math inline">\(\frac{\partial E}{\partial l_l} =
E&#39;\)</span>，这个值由loss函数决定。 因此 <span class="math display">\[
\begin{aligned}
dE &amp;= E&#39;^Tdl_l\\
&amp;=E&#39;^T(\sigma_l&#39;(z_l) \odot (dz_l))\\
&amp;=E&#39;^Tdiag(\sigma_l&#39;(z_l))da_l\\
\Rightarrow \frac{\partial E}{\partial z_l} &amp;=
diag(\sigma_l&#39;(z_l))E&#39;
\end{aligned}
\]</span></p>
<p>这里把<span class="math inline">\(\frac{\partial E}{\partial
z_l}\)</span>记作<span class="math inline">\(\delta_l\)</span></p>
<p>因为： <span class="math display">\[
\begin{aligned}
    da_i &amp;= w_idl_{i-1}\\
    &amp;= w_i(\sigma_i&#39;(z_{i-1}) \odot (dz_{i-1}))\\
    &amp;=w_idiag(\sigma_{i-1}&#39;(z_{i-1}))dz_{i-1}\\
    \Rightarrow \frac{\partial z_i}{\partial z_{i-1}} &amp;=
diag(\sigma_{i-1}&#39;(z_{i-1}))w_i^T\\
\end{aligned}
\]</span></p>
<p>所以定义: <span class="math display">\[
\begin{aligned}
    \delta_i &amp;= \frac{\partial E}{\partial z_i},\ i=1,2,...,l-1\\
    \Rightarrow \delta_{i-1} &amp;= \frac{\partial z_i}{\partial
z_{i-1}}\frac{\partial E}{\partial z_i},\ i=2,...,l\\
    &amp;= diag(\sigma_{i-1}&#39;(z_{i-1}))w_i^T\delta_i\\
\end{aligned}
\]</span></p>
<p>现在再来考虑<span class="math inline">\(E\)</span>对<span class="math inline">\(w_{l-k}\)</span>的导数： <span class="math display">\[
\begin{aligned}
    dE &amp;= \frac{\partial E}{\partial z_{l-k}}^Tdz_{l-k}\\
    &amp;= \delta_{l-k}^T(dw_{l-k}l_{l-k-1} + db_{l-k})\\
    &amp;= tr(\delta_{l-k}^Tdw_{l-k}l_{l-k-1} +
\delta_{l-k}^Tdb_{l-k})\\
    &amp;= tr(l_{l-k-1}\delta_{l-k}^Tdw_{l-k} +
\delta_{l-k}^Tdb_{l-k})\\
    \Rightarrow \frac{\partial E}{\partial w_{l-k}} &amp;=
\delta_{l-k}l_{l-k-1}^T\\
    \Rightarrow \frac{\partial E}{\partial b_{l-k}} &amp;= \delta_{l-k}
\end{aligned}
\]</span> 这里的变换属于标量对矩阵求导<span class="math inline">\(d f =
tr((\frac{\partial f}{\partial X}) ^ T
dX)\)</span>，且用到了迹的一个性质：<span class="math inline">\(tr(A B)
= tr(B A)\)</span>，其中<span class="math inline">\(A\)</span>和<span class="math inline">\(B^T\)</span>大小相同</p>
<p>全连接层的BP算法看起来很复杂，其实非常简单，只要使用以下几个等式即可求出任一层的权重和偏置的导数：
<span class="math display">\[
\begin{aligned}
    \delta_l = \frac{\partial E}{\partial z_l} &amp;=
diag(\sigma_l&#39;(z_l))E&#39;\\
    \delta_{i} = \frac{\partial E}{\partial z_i} &amp;=
diag(\sigma_i&#39;(z_i))w_{i+1}^T\delta_{i+1},\ i=1,2,...,l-1\\
    \frac{\partial E}{\partial w_i} &amp;= \delta_il_{i-1}^T,\
i=1,2,...,l\\
    \frac{\partial E}{\partial b_i} &amp;= \delta_i,\ i=1,2,...,l
\end{aligned}
\]</span></p>
<h2 id="rbfradial-basis-function径向基函数网络">RBF（Radial Basis
Function，径向基函数）网络</h2>
<p>RBF网络是指使用径向基函数作为隐层激活函数的单隐层前馈神经网络<span class="math inline">\(\phi(x) = \sum\limits_{i=1}^q w_i\rho(x,
c_i)\)</span>，常用的高斯径向基函数<span class="math inline">\(\rho(x,
c_i) = e^{-\beta_i||x-c_i||^2}\)</span>，其中<span class="math inline">\(c_i,w_i\)</span>分别表示第<span class="math inline">\(i\)</span>个神经元的中心和权重。</p>
<p><span class="math inline">\(c_i\)</span>可以使用随机采样或者聚类来获得，而其他参数<span class="math inline">\(w_i, \beta_i\)</span>由BP算法等方法来获得。</p>
<h1 id="svm">SVM</h1>
<p>支持向量机的相关内容可以见我的另一篇文章<a href="#">Post not found: SVM学习笔记 SVM学习笔记</a>，这里不再重新做笔记。</p>
<h1 id="贝叶斯分类器">贝叶斯分类器</h1>
<h2 id="条件风险">条件风险</h2>
<p>条件风险<span class="math inline">\(R(c_i|x) = \sum\limits_{j=1}^N
\lambda_{ij}P(c_j|x)\)</span>其中<span class="math inline">\(\lambda_{ij}\)</span>表示将<span class="math inline">\(j\)</span>类样本分类为<span class="math inline">\(i\)</span>时的损失。</p>
<p>机器学习的任务是寻找一个判定准则<span class="math inline">\(h:x
\rightarrow y\)</span>以最小化总体风险<span class="math inline">\(\min\limits_{h}
R(h)=E_x[R(h(x)|x)]\)</span>，即在每个样本上选择<span class="math inline">\(h^\star(x) = \mathop{\arg\max}\limits_c
R(c|x)\)</span>，这样的分类器<span class="math inline">\(h(x)\)</span>被称为贝叶斯最优分类器。与之对应的总体风险<span class="math inline">\(R(h^\star)\)</span>被称为贝叶斯风险，<span class="math inline">\(1-R(h^\star)\)</span>是分类器能达到的最好性能，即通过机器学习能产生的模型精度理论上限。</p>
<p>如果<span class="math inline">\(\lambda_{ij} = \begin{cases}1&amp;i
\ne j\\0&amp;i = j\end{cases}\)</span>，那么条件风险将变成<span class="math inline">\(R(c|x) =
1-P(c|x)\)</span>，于是最小化分类错误率的贝叶斯最优分类器变成<span class="math inline">\(h^\star(x) = \mathop{\arg\max}\limits_c
P(c|x)\)</span>即最大化后验概率。</p>
<h2 id="生成式与判别式模型">生成式与判别式模型</h2>
<p>在使用贝叶斯分类器时，需要获取后验概率<span class="math inline">\(P(c|x)\)</span>，但是非常难以直接获取，因此有两种方式：</p>
<p>第一种是直接对<span class="math inline">\(P(c|x)\)</span>进行建模，称为判别式方法。</p>
<p>第二种是生成式模型，考虑<span class="math inline">\(P(c|x) =
\frac{P(x, c)}{P(x)}=\frac{P(c)P(x|c)}{p(x)}\)</span>，其中<span class="math inline">\(P(c)\)</span>称为类先验概率，<span class="math inline">\(P(x|c)\)</span>是样本<span class="math inline">\(x\)</span>相对于类别<span class="math inline">\(c\)</span>的类条件概率（似然），<span class="math inline">\(P(c)\)</span>可以在样本量足够大时用各类样本出现的频率来估计（大数定律），但是<span class="math inline">\(P(x|c)\)</span>非常难估计，因为这涉及到关于<span class="math inline">\(x\)</span>的所有属性的联合概率，很难直接用样本出现的频率来进行估计。</p>
<h2 id="极大似然估计">极大似然估计</h2>
<p>再生成式模型中，估计<span class="math inline">\(P(x|c)\)</span>的一种策略是首先确定其满足某种确定的概率分布形式，假设<span class="math inline">\(P(x|c)\)</span>被参数向量<span class="math inline">\(\theta_c\)</span>唯一确定，因此<span class="math inline">\(P(x|c)\)</span>可以被记为<span class="math inline">\(P(x|\theta_c)\)</span>，概率模型的训练过程就是参数估计的过程，参数估计有两种方案：</p>
<ul>
<li>参数虽然未知，但是却是客观存在的固定值（频率主义学派）</li>
<li>参数也是一种未观察到的随机变量，其本身也满足一定的分布，因此需要先假设参数服从一个先验分布，然后基于观测数据来计算参数的后验分布（贝叶斯学派）</li>
</ul>
<p>极大似然估计属于频率主义学派，将参数当成未知的固定值来处理。首先令<span class="math inline">\(D_c\)</span>表示训练集<span class="math inline">\(D\)</span>的第<span class="math inline">\(c\)</span>类样本集合，并假设这些样本独立同分布，因此其似然可以表示为<span class="math inline">\(P(D_c|\theta_c) = \prod\limits_{x \in
D_c}P(x|\theta_c)\)</span>，极大化似然，就能找到参数<span class="math inline">\(\theta_c\)</span>。</p>
<p>似然的表达式中有连乘，容易造成下溢，因此通常使用对数似然<span class="math inline">\(\log P(D_c|\theta_c) = \sum\limits_{x \in D_c}
\log P(x|\theta_c)\)</span>。</p>
<p>像极大似然法这种参数化的方法的准确性严重依赖于对<span class="math inline">\(P(x|\theta_c)\)</span>分布的假设，在实际应用中需要利用应用任务的经验知识，才能得到比较好的分类器。</p>
<h2 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h2>
<p>在生成式模型中，<span class="math inline">\(P(c|x) = \frac{P(x,
c)}{P(x)}=\frac{P(c)P(x|c)}{P(x)}\)</span>，这里估计<span class="math inline">\(P(x|c)\)</span>的困难在于类条件概率<span class="math inline">\(P(x|c)\)</span>是<span class="math inline">\(x\)</span>所有属性的联合分布，难以从有限的训练样本中估计得到。</p>
<p>朴素贝叶斯分类器采用属性条件独立性假设：对所有已知类别，样本<span class="math inline">\(x\)</span>的所有属性相互独立。</p>
<p>因此<span class="math inline">\(P(c|x) = \frac{P(c)P(x|c)}{P(x)} =
\frac{P(c)}{P(x)} \prod\limits_{i=1}^dP(x_i|c)\)</span>，其中<span class="math inline">\(d\)</span>为样本的属性数量，<span class="math inline">\(x_i\)</span>表示样本<span class="math inline">\(x\)</span>的第<span class="math inline">\(i\)</span>个属性值。</p>
<p>对于所有类别来说，<span class="math inline">\(P(x)\)</span>相同，因此基于<span class="math inline">\(\lambda_{ij} = \begin{cases}1&amp;i \ne j\\0&amp;i
= j\end{cases}\)</span>，<span class="math inline">\(h^\star(x) =
\mathop{\arg\max}\limits_c
P(c|x)\)</span>即最大化后验概率的朴素贝叶斯分类器就可以表达为<span class="math inline">\(h_{nb}(x) = \mathop{\arg\max}\limits_c
P(c)\prod\limits_{i=1}^dP(x_i|c)\)</span></p>
<p>对于类先验概率，可以从训练集中使用<span class="math inline">\(P(c) =
\frac{|D_c|}{|D|}\)</span>估计。</p>
<p>对于离散属性，估计<span class="math inline">\(P(x_i|c)\)</span>的方式常用<span class="math inline">\(P(x_i|c) =
\frac{|D_{c,x_i}|}{|D_c|}\)</span>，其中<span class="math inline">\(D_{c,x_i}\)</span>表示<span class="math inline">\(D_c\)</span>中第<span class="math inline">\(i\)</span>个属性取值为<span class="math inline">\(x_i\)</span>的样本集合。</p>
<p>对于连续属性，则可以使用概率密度函数，假定<span class="math inline">\(P(x_i|c)\)</span>服从某种分布，然后对其进行估计。</p>
<p>在离散属性的处理上，有个问题是：如果训练集中某种属性在类别<span class="math inline">\(c\)</span>上没有出现，或者类别<span class="math inline">\(c\)</span>在训练集上没有出现，则<span class="math inline">\(P(c)\prod\limits_{i=1}^dP(x_i|c)\)</span>直接就为0了，因此需要进行修正平滑处理。</p>
<p>常用的是拉普拉斯修正，即将<span class="math inline">\(P(c) =
\frac{|D_c|}{|D|}\)</span>更改为<span class="math inline">\(P(c) =
\frac{|D_c|+1}{|D|+N}\)</span>，其中N表示类别个数, 将<span class="math inline">\(P(x_i|c) =
\frac{|D_{c,x_i}|}{|D_c|}\)</span>更改为<span class="math inline">\(P(x_i|c) = \frac{|D_{c,x_i}| + 1}{|D_c| +
N_i}\)</span>，其中<span class="math inline">\(N_i\)</span>表示第<span class="math inline">\(i\)</span>个属性的可取值个数。这样可以避免因训练集样本不充分而导致的概率估计为零的问题。</p>
<h2 id="半朴素贝叶斯分类器">半朴素贝叶斯分类器</h2>
<p>由于属性条件独立性假设很难成立，因此尝试对条件独立性假设进行一定程度的放松。</p>
<p>例如独依赖估计（One-Dependent
Estimator，ODE），即每个属性在类别之外最多依赖一个其他属性：<span class="math inline">\(P(c|x)\propto P(c)
\prod\limits_{i=1}^dP(x_i|c,pa_i)\)</span>，其中<span class="math inline">\(pa_i\)</span>为属性<span class="math inline">\(x_i\)</span>所依赖的属性，称为<span class="math inline">\(x_i\)</span>的父属性。</p>
<p>问题的关键在于如何确定父属性，最直接的办法是假设所有属性都依赖于同一个父属性，称为超父（super-parent）由此形成了SPODE(Super-Parent
ODE)方法。</p>
<h1 id="集成学习">集成学习</h1>
<p>构建并结合多个学习器来完成学习任务。一般是先产生一组个体学习器，然后使用某种策略将他们结合。</p>
<p>根据个体学习器的生成方式，集成学习方法大致可以分为两类： -
个体学习器之间存在强依赖关系，必须串行生成，例如：Boosting。 -
个体学习器之间不存在强依赖关系，可以同时并行生成，例如：Bagging和随机森林（Random
Forest）</p>
<h2 id="boosting">Boosting</h2>
<p>先训练一个基学习器，然后使用基学习器对训练样本分布进行调整，使得基学习器预测错误的样本在后续得到更多关注，然后进行下一个基学习器的训练，直到学习器数量达到指定数量。</p>
<p>Boosting中最著名的是AdaBoosting，可以理解为加性模型，即使用基学习器的线性组合<span class="math inline">\(H(x) = \sum\limits_{t=1}^T\alpha_t h_t
(x)\)</span>来最小化指数损失函数<span class="math inline">\(l_{exp}(H|D)
= E_{x \sim D}[e^{-f(x)H(x)}]\)</span>，其中<span class="math inline">\(h_i\)</span>表示第<span class="math inline">\(i\)</span>个基分类器，<span class="math inline">\(f(x)\)</span>表示真实函数<span class="math inline">\(f(x) = y\)</span>。</p>
<p>AdaBoosting只适用于二分类任务。</p>
<h2 id="bagging">Bagging</h2>
<p>在原始数据中每次有放回的采样<span class="math inline">\(m\)</span>个样本，组成一个子训练集，最终得到共<span class="math inline">\(T\)</span>个子训练集，用每个子训练集训练一个基学习器，再将所有基学习器进行结合。</p>
<p>假设基学习器的计算复杂度为<span class="math inline">\(O(m)\)</span>，Bagging的复杂度大致为<span class="math inline">\(T(O(m) + O(s))\)</span>，其中<span class="math inline">\(O(s)\)</span>为投票和采样的复杂度，一般非常小，而<span class="math inline">\(T\)</span>是个比较小的常数，因此Bagging的训练复杂度和训练一个学习器的复杂程度同阶，非常高效。</p>
<h2 id="bagging和boosting的区别">Bagging和Boosting的区别</h2>
<h2 id="随机森林random-forestrf">随机森林（Random Forest，RF）</h2>
<p>以决策树为基学习器的Bagging集成基础上，引入随机属性选择。在构造决策树过程中，选择最优划分属性时，可以在当前可用属性的随机子集中进行选择。</p>
<h2 id="集成学习的结合策略">集成学习的结合策略</h2>
<ul>
<li>平均法</li>
<li>投票法</li>
<li>学习法</li>
</ul>
<p>stacking是学习法的一个例子：先用原始训练集训练出初级学习器，然后将初级学习器的输出作为输入特征，训练一个次级学习器，用于结合初级学习器输出得到最终输出。最好在初级学习器的验证集上对次级学习器进行训练。</p>
<h2 id="多样性">多样性</h2>
<p>如果用<span class="math inline">\(h_1,h_2,...,h_T\)</span>通过加权平均法来集成得到<span class="math inline">\(H\)</span>来估计真实函数<span class="math inline">\(f:R^d \rightarrow R\)</span>，则对样本<span class="math inline">\(x\)</span>，定义学习器<span class="math inline">\(h_i\)</span>的分歧为：<span class="math inline">\(A(h_i|x) = (h(x_i) -
H(x))^2\)</span>。集成的分歧表示为<span class="math inline">\(\overline{A}(H|x) = \sum\limits_{i=1}^T w_i
A(h_i|x)=\sum\limits_{i=1}^Tw_i(h_i(x) -
H(x))^2\)</span>。分歧表示个体学习器在样本<span class="math inline">\(x\)</span>上的不一致性。</p>
<p>而集成的泛化误差可以表示为<span class="math inline">\(E=\overline{E}
- \overline{A}\)</span>（<span class="math inline">\(\overline{E} =
\sum\limits_{i=1}^Tw_iE_i\)</span>，<span class="math inline">\(\overline{A}=\sum_{i=1}^Tw_iA_i\)</span>，推导过程略过），这说明个体学习器误差越低，多样性越大，集成的效果越好。</p>
<p>增加个体学习器多样性的方法： -
数据样本扰动：通常基于采样法，产生不同的数据子集。但是有些学习器对数据样本量的扰动不敏感（如线性学习器，支持向量机，朴素贝叶斯，k近邻，这些学习器称为稳定基学习器）
- 输入属性扰动：抽取属性子集来训练学习器。不适合属性较少的数据。 -
输出表示扰动：将样本的类别标记稍作改动，例如随机改变一些标记，或者进行编码调制，例如ECOC。
-
算法参数扰动：随机设置不同的参数，例如隐层神经元个数、初始连接权值等。</p>
<h1 id="聚类">聚类</h1>
<h2 id="原型聚类">原型聚类</h2>
<p>又称基于原型的聚类方法，此类算法假设聚类结构可以通过一组原型来刻画，通常先对原型进行初始化，然后对原型进行迭代更新求解。不同的原型表示、不同的求解方式就会产生不同的算法</p>
<h3 id="k均值算法">k均值算法</h3>
<p>给定样本集<span class="math inline">\(D=\{x_1,x_2,...,x_m\}\)</span>，聚类结果<span class="math inline">\(C = \{C_1, C_2,
...,C_k\}\)</span>，定义平方误差<span class="math inline">\(E =
\sum\limits_{i=1}^k \sum\limits_{x\in C_i}
||x-\mu_i||^2_2\)</span>，其中<span class="math inline">\(\mu_i =
\frac{1}{C_i}\sum\limits_{x \in C_i}x\)</span>表示簇<span class="math inline">\(C_i\)</span>的均值向量。</p>
<p>k均值算法针对平方误差<span class="math inline">\(E\)</span>进行优化，使用贪心策略，通过迭代优化方式来进行：</p>
<p>1、初始化<span class="math inline">\(k\)</span>个初始均值向量<span class="math inline">\(\mu_1, \mu_2, ..., \mu_k\)</span>。</p>
<p>2、计算每个样本到<span class="math inline">\(k\)</span>个均值向量的值，将每个样本划入最近的均值向量对应的簇中，得到一个划分。</p>
<p>3、使用划分好的簇计算新的均值向量。</p>
<p>4、如果没有均值向量被大幅更新或者达到了最大迭代次数，那么停止，否则从第2步继续循环。</p>
<h3 id="学习向量量化learning-vector-quantizationlvq">学习向量量化（Learning
Vector Quantization，LVQ）</h3>
<p>和k均值算法类似，但是学习向量量化在学习过程中还利用样本的监督信息来辅助聚类：</p>
<p>1、初始化原型向量<span class="math inline">\(\{p_1, p_2, ...,
p_q\}\)</span></p>
<p>2、随机选取样本<span class="math inline">\(x_j\)</span>，计算<span class="math inline">\(x_j\)</span>和每个原型向量的距离，并找出距离最小对应的原型向量。</p>
<p>3、如果<span class="math inline">\(x_j\)</span>的标签和原型向量的标签相同，则使用<span class="math inline">\(p=p + \eta (x_j -
p)\)</span>来对选出的原型向量进行更新，否则使用<span class="math inline">\(p = p - \eta (x_j -
p)\)</span>来对选出的原型向量进行更新</p>
<p>4、达到最大迭代轮数或者更新幅度很小则停止更新，否则从第2步继续循环。</p>
<p>其中<span class="math inline">\(\eta \in (0,
1)\)</span>表示学习速率，在迭代停止之后，对于任意样本<span class="math inline">\(x\)</span>，可以将其划分到与其距离最近的原型向量所代表的的簇中。</p>
<h3 id="高斯混合聚类mixture-of-gaussian">高斯混合聚类（Mixture-of-Gaussian）</h3>
<p>假设数据服从高斯混合分布<span class="math inline">\(p_M(x) =
\sum\limits_{i=1}^k \alpha_i p(x|\mu_i, \Sigma_i), \sum\limits_{i=1}^k
\alpha_i=1\)</span>。</p>
<p>令随机变量<span class="math inline">\(z_j,j\in
\{1,2,...,k\}\)</span>表示样本<span class="math inline">\(x_j\)</span>预测类别，其先验概率<span class="math inline">\(p(z_j = i) = \alpha_i\)</span>，则<span class="math inline">\(p_M(z_j = i|x_j) =
\frac{\alpha_ip(x_i|\mu_i,\Sigma_i)}{\sum\limits_{l=1}^k
\alpha_lp(x_j|\mu_l, \Sigma_l)}\)</span></p>
<p>当高斯混合分布已知时，即可将样本划分成<span class="math inline">\(k\)</span>个簇，其标记为<span class="math inline">\(\mathop{\arg\max}\limits_i p_M(z_j =
i|x_j)\)</span></p>
<p>估计这样的高斯混合分布可以使用极大似然法，令对数似然<span class="math inline">\(LL(p_M) = \sum\limits_{j=1}^m
ln(\sum\limits_{i=1}^k
\alpha_ip(x_i|\mu_i,\Sigma_i))\)</span>，使用EM算法即可求解。</p>
<h2 id="密度聚类">密度聚类</h2>
<h3 id="dbscan">DBSCAN</h3>
<p>密度聚类算法的代表是DBSCAN算法，使用一组邻域参数<span class="math inline">\((\epsilon,
MinPts)\)</span>来刻画样本分布的紧密程度。</p>
<ul>
<li><span class="math inline">\(\epsilon\)</span>-邻域：<span class="math inline">\(N_\epsilon(x_j) = \{x_i | x_i \in D,
dist(x_j,x_i)\le \epsilon\}\)</span>，其中<span class="math inline">\(D\)</span>表示全部数据集合。</li>
<li>核心对象：满足<span class="math inline">\(|N_\epsilon(x_j)| \ge
MinPts\)</span>的<span class="math inline">\(x_j\)</span>称为核心对象，其中<span class="math inline">\(|N_\epsilon(x_j)|\)</span>表示<span class="math inline">\(x_j\)</span>的<span class="math inline">\(\epsilon\)</span>-邻域中的样本数量。</li>
<li>密度直达：<span class="math inline">\(x_j\)</span>是一个核心对象，且<span class="math inline">\(x_i\)</span>在<span class="math inline">\(x_j\)</span>的<span class="math inline">\(\epsilon\)</span>-邻域中，则<span class="math inline">\(x_i\)</span>由<span class="math inline">\(x_j\)</span>密度直达。</li>
<li>密度可达：如果存在样本序列<span class="math inline">\(p_1, p_2, ...,
p_n, p_1 = x_j, p_n = x_i\)</span>使得<span class="math inline">\(p_{m+1}\)</span>由<span class="math inline">\(p_m\)</span>密度直达，则称<span class="math inline">\(x_i\)</span>由<span class="math inline">\(x_j\)</span>密度可达。</li>
<li>密度相连：如果存在<span class="math inline">\(x_k\)</span>使得<span class="math inline">\(x_j\)</span>和<span class="math inline">\(x_i\)</span>均可由<span class="math inline">\(x_k\)</span>密度可达，则称<span class="math inline">\(x_i\)</span>和<span class="math inline">\(x_j\)</span>密度相连。</li>
</ul>
<p>基于以上概念，DBSCAN将簇定义为由密度可达关系到处的最大密度相连样本集合。</p>
<p>DBSCAN的聚类过程如下：</p>
<p>1、找到所有的核心对象。</p>
<p>2、循环随机取一个还没有访问过的核心对象，将其所有密度相连的样本生成一个簇并标记为已访问，如果没有未访问的核心对象，则停止循环</p>
<p>3、仍未访问的样本被视为噪声样本。</p>
<h2 id="层次聚类">层次聚类</h2>
<p>层次聚类方法试图在不同层次上对数据进行划分，形成树形的聚类结构。</p>
<h3 id="agnes">AGNES</h3>
<p>AGNES是一种自底向上的层次聚类算法，首先将所有样本单独看做一个簇，然后每次迭代找到距离最近的两个簇进行合并，直到簇数量等于指定数量。</p>
<p>这里的关键是如何定义两个簇的距离，主要有三种方式，使用三种距离计算的AGNES分别被称为：
- 最大距离：单链接算法 - 最小距离：全链接算法 - 平均距离：均链接算法</p>
<h1 id="降维与度量学习">降维与度量学习</h1>
<h2 id="k近邻k-nearest-neighbor学习">k近邻（k-Nearest
Neighbor）学习</h2>
<p>k近邻方法没有训练过程，在给定测试样本时，直接使用训练样本中与其最靠近的<span class="math inline">\(k\)</span>个样本，基于这<span class="math inline">\(k\)</span>个样本的信息来对测试样本进行预测。</p>
<p>k近邻方法是懒惰学习（lazy
learning）的一个代表，而那些在训练阶段就对样本进行学习处理的方法称为急切学习（eager
learning）</p>
<h2 id="低维嵌入">低维嵌入</h2>
<p>大部分时候，观测到的数据是高维数据，但与学习任务密切相关的很可能是高维空间中的一个低维嵌入。</p>
<h3 id="多维缩放multiple-dimensional-scalingmds">多维缩放（Multiple
Dimensional Scaling，MDS）</h3>
<p>多维缩放的思路是找到一个低维空间，使得在这个低维空间中的欧氏距离和原始空间中的距离相等。</p>
<p>假如原始空间的维度为<span class="math inline">\(d\)</span>，所有数据的距离矩阵<span class="math inline">\(D\in R^{m \times m}\)</span>，其中<span class="math inline">\(m\)</span>为样本数量，<span class="math inline">\(d_{ij}\)</span>表示样本<span class="math inline">\(x_i\)</span>和<span class="math inline">\(x_j\)</span>的距离，降维后的数据<span class="math inline">\(z \in R^{d&#39;}\)</span>，所有样本表示为<span class="math inline">\(Z\in R^{d&#39; \times m}\)</span>。</p>
<p>令<span class="math inline">\(B = Z^T Z \in R^{m\times
m}\)</span>是降维后的内积矩阵，有<span class="math inline">\(b_{ij} =
z_i^T z_j\)</span></p>
<p>则<span class="math inline">\(d_{ij}^2 = ||z_i||^2 + ||z_j||^2 -
2z_i^T z_j = b_{ii} + b_{jj} - 2b_{ij}\)</span></p>
<p>令降维后的样本<span class="math inline">\(Z\)</span>被中心化，即<span class="math inline">\(\sum\limits_{i=1}^m z_i =
\mathbf{0}\)</span>，可得到<span class="math inline">\(\sum\limits_{i=1}^m b_{ij} = \sum\limits_{j=1}^m
b_{ij} = 0\)</span></p>
<p>因此：</p>
<p><span class="math display">\[
\sum\limits_{i=1}^md_{ij}^2 = tr(B) + mb_{jj}\\
\sum\limits_{j=1}^md_{ij}^2 = tr(B) + mb_{ii}\\
\sum\limits_{i=1}^m\sum\limits_{j=1}^md_{ij}^2 = 2m\ tr(B)\\
tr(B) = \sum\limits_{i=1}^m b_{ii}
\]</span></p>
<p>则有：</p>
<p><span class="math display">\[
\begin{aligned}
    b_{ij} &amp;= \frac{b_{ii} + b_{jj} - d_{ij}^2}{2}\\
    &amp;=\frac{1}{2m}(\sum\limits_{i=1}^md_{ij}^2) +
\frac{1}{2m}(\sum\limits_{j=1}^md_{ij}^2) - \frac{1}{2m^2}
\sum\limits_{i=1}^m\sum\limits_{j=1}^md_{ij}^2 - \frac{d_{ij}^2}{2}
\end{aligned}
\]</span></p>
<p>这样就能根据原来的距离矩阵求出内积矩阵<span class="math inline">\(B\)</span>，再对<span class="math inline">\(B\)</span>进行特征分解得到<span class="math inline">\(B=V \Lambda V^T\)</span>，其中<span class="math inline">\(\Lambda = diag(\lambda_1, ...,
\lambda_d)\)</span>为特征值构成的对角阵，<span class="math inline">\(\lambda_1 \ge \lambda_2\ge ...\ge
\lambda_d\)</span>，为了降维，我们可以取其中<span class="math inline">\(d&#39;\)</span>个非零值，构成对角矩阵<span class="math inline">\(\Lambda_\star\)</span>与其对应的特征向量矩阵<span class="math inline">\(V_\star\)</span>，则<span class="math inline">\(Z
= \Lambda_\star ^{\frac{1}{2}} V_\star^T \in R^{d&#39; \times
m}\)</span></p>
<h2 id="主成分分析pca">主成分分析（PCA）</h2>
<p>如果希望使用一个超平面来对数据进行表示，那么可以从两个方面去考虑： -
最近重构性：样本点到这个超平面的距离都足够近 -
最大可分性：样本点在这个超平面上的投影尽可能分开</p>
<p>但是两个方面的考虑最终都会得到PCA的等价推导，即PCA既保证了最近重构性也保证了最大可分性。</p>
<p>假定数据<span class="math inline">\(x_i \in
R^d\)</span>已经进行过中心化，即<span class="math inline">\(\sum_i x_i =
\mathbf{0}\)</span>，现在使用一组标准正交基对<span class="math inline">\(x_i\)</span>进行投影，得到<span class="math inline">\(z_i = Wx_i, W = \begin{bmatrix}w_1^T\\ w_2^T\\
\vdots\\ w_{d&#39;}^T \end{bmatrix}\in R^{d&#39; \times d}, w_i^T w_j =
\begin{cases}1 &amp; i=j\\ 0&amp; i\ne j\end{cases}\)</span>，其中<span class="math inline">\(z_{ij} = w_j^Tx_i\)</span>，如果使用<span class="math inline">\(z_i\)</span>来还原<span class="math inline">\(x_i\)</span>则得到<span class="math inline">\(\hat{x}_i = \sum\limits_{j=1}^{d&#39;}z_{ij}w_j =
W^T z_i\)</span>。</p>
<p>如果从最近重构性来考虑，我们希望最小化<span class="math inline">\(\sum\limits_{i=1}^m ||\hat{x}_i -
x_i||^2_2\)</span>，即： <span class="math display">\[
\begin{aligned}
    &amp;\min \sum\limits_{i=1}^m ||\hat{x}_i - x_i||^2_2\\
    &amp;=\min \sum\limits_{i=1}^m ||
\sum\limits_{j=1}^{d&#39;}z_{ij}w_j - x_i||^2_2\\
    &amp;=\min \sum\limits_{i=1}^m || W^Tz_i - x_i||^2_2\\
    &amp;=\min \sum\limits_{i=1}^m (z_i^TWW^Tz_i - z_i^TWx_i -
x^T_iW^Tz_i + x^T_i x_i)\\
    &amp;=\min \sum\limits_{i=1}^m (z_i^Tz_i - z_i^TWx_i -
x^T_iW^Tz_i)\\
    &amp;=\min \sum\limits_{i=1}^m tr(z_i^Tz_i - z_i^TWx_i -
x^T_iW^Tz_i)\\
    &amp;=\min \sum\limits_{i=1}^m tr(z_i^Tz_i - 2z_i^TWx_i)\\
    &amp;=\min \sum\limits_{i=1}^m -tr(z_i^T z_i)\\
    &amp;=\min -tr(Z^T Z),\ Z = \begin{bmatrix}z_1 &amp;z_2 &amp; \cdots
&amp;z_m\end{bmatrix} = WX,\ X = \begin{bmatrix}x_1 &amp;x_2 &amp;
\cdots &amp;x_m\end{bmatrix}\\
    &amp;=\min -tr(X^TW^TWX)\\
    &amp;=\min -tr(WXX^TW^T)\\
\end{aligned}
\]</span></p>
<p>因此我们需要解决的问题就是： <span class="math display">\[
\begin{aligned}
    \min\ &amp; -tr(WXX^TW^T)\\
    s.t.\ &amp; WW^T = I_{d&#39;}
\end{aligned}
\]</span></p>
<p>另一方面，如果从最大可分性来考虑，我们希望最大化<span class="math inline">\(z_i\)</span>之间的方差<span class="math inline">\(\sum\limits_{i=1}^m (z_i -
\frac{1}{m}\sum\limits_{j=1}^mz_j)^2\)</span></p>
<p><span class="math display">\[
\begin{aligned}
    &amp;\max \sum\limits_{i=1}^m ||z_i -
\frac{1}{m}\sum\limits_{j=1}^mz_j||_2^2\\
    &amp;=\max \sum\limits_{i=1}^m ||z_i -
\frac{1}{m}\sum\limits_{j=1}^mWx_j||_2^2\\
    &amp;=\max \sum\limits_{i=1}^m ||z_i -
\frac{1}{m}W\sum\limits_{j=1}^mx_j||_2^2\\
    &amp;=\max \sum\limits_{i=1}^m ||z_i||_2^2\\
    &amp;=\max \sum\limits_{i=1}^m (z^T_iz_i)\\
    &amp;=\max\ tr(Z^TZ)\\
    &amp;=\max\ tr(X^TW^TWX)\\
    &amp;=\min\ -tr(X^TW^TWX)\\
    &amp;=\min\ -tr(WXX^TW^T)\\
\end{aligned}
\]</span></p>
<p>因此我们需要解决的问题就是： <span class="math display">\[
\begin{aligned}
    \min\ &amp; -tr(WXX^TW^T)\\
    s.t.\ &amp; WW^T = I_{d&#39;}\\
\end{aligned}
\]</span></p>
<p>由此可见，在<span class="math inline">\(\sum x_i =
0\)</span>的情况下，从两个方面得到的结果完全相同。</p>
<p>求解PCA可以使用拉格朗日法，首先得到拉格朗日函数<span class="math inline">\(L(W) = -tr(X^TW^TWX) + \lambda (WW^T -
I_{d&#39;}), \lambda \ge 0\)</span></p>
<p><span class="math display">\[
\begin{aligned}
    dL(W) &amp;= -tr(WXX^TdW^T + dWXX^TW^T) + \lambda tr(dW W^T +
WdW^T)\\
    &amp;= -tr(2XX^TW^TdW - 2\lambda W^TdW)\\
    \\
    \frac{\partial dL(W)}{\partial W} &amp;= 2\lambda W - 2WXX^T
\end{aligned}
\]</span></p>
<p>令<span class="math inline">\(\frac{\partial dL(W)}{\partial W} =
0\)</span>可得<span class="math inline">\(\lambda W^T =
XX^TW^T\)</span>，即求出协方差矩阵<span class="math inline">\(XX^T\)</span>的特征向量即可构成<span class="math inline">\(W^T\)</span>，在这个过程中，可以舍弃一部分特征向量，只取特征值最大的<span class="math inline">\(d&#39;\)</span>个特征向量，即可将数据维度从<span class="math inline">\(d\)</span>维缩减到<span class="math inline">\(d&#39;\)</span>维。</p>
<p>如果将<span class="math inline">\(X\)</span>进行奇异值分解，则有<span class="math inline">\(X=D\Sigma V^T\)</span>，<span class="math inline">\(XX^T = D \Sigma \Sigma^T D^T\)</span>，其中<span class="math inline">\(D\)</span>是<span class="math inline">\(X\)</span>的左奇异矩阵，也就是<span class="math inline">\(XX^T\)</span>的特征矩阵，如果令<span class="math inline">\(W=D^T\)</span>可以得到<span class="math inline">\(Z=D^TX=D^TD\Sigma V^T = \Sigma
V^T\)</span>，因此求出<span class="math inline">\(X^TX\)</span>的特征矩阵也可以求出<span class="math inline">\(Z\)</span>。</p>
<h2 id="核化线性降维">核化线性降维</h2>
<h3 id="核主成分分析kpca">核主成分分析（KPCA）</h3>
<p>PCA是一种线性降维方式，在降维时假设从高维空间到低维空间的映射是线性的，其线性映射由<span class="math inline">\(W\)</span>确定，且有<span class="math inline">\(\lambda W^T = XX^TW^T = (\sum\limits_{i=1}^m x_i
x^T_i)W^T\)</span>，假设在高维空间<span class="math inline">\(\phi(x)\)</span>中进行PCA，则有<span class="math inline">\(\lambda W^T = (\sum\limits_{i=1}^m \phi(x_i)
\phi(x_i)^T)W^T\)</span>，有如下推导：</p>
<p><span class="math display">\[
\begin{aligned}
    \lambda W^T &amp;= (\sum\limits_{i=1}^m \phi(x_i) \phi(x_i)^T)W^T\\
    &amp;= (\sum\limits_{i=1}^m \phi(x_i) \phi(x_i)^T)W^T\\
    &amp;= \sum\limits_{i=1}^m \phi(x_i)\phi(x_i)^TW^T\\
    &amp;= \sum\limits_{i=1}^m\phi(x_i)\alpha_i,\ \alpha_i =
\phi(x_i)^TW^T\\
\end{aligned}
\]</span></p>
<p>由于<span class="math inline">\(\phi\)</span>函数无法明确求出，因此引入核函数。
<span class="math display">\[
\begin{aligned}
    \lambda \phi(x_j)^TW^T &amp;=
\phi(x_j)^T\sum\limits_{i=1}^m\phi(x_i)\alpha_i\\
    \lambda A&amp;=KA\\
\end{aligned}
\]</span></p>
<p>其中<span class="math inline">\(K\)</span>是核矩阵<span class="math inline">\(K_{ij} = \phi(x_i)^T\phi(x_j)\)</span>，<span class="math inline">\(A = \begin{bmatrix}\alpha_1, \alpha_2, ...,
\alpha_m\end{bmatrix}\)</span></p>
<p>则<span class="math inline">\(z_{ij} = w_j^T \phi(x_i) =
\sum\limits_{k=1}^m\alpha_{k,j}\phi(x_k)^T\phi(x_i)=\sum\limits_{k=1}^m\alpha_{k,j}K_{ki}\)</span>表示降维之后<span class="math inline">\(x_i\)</span>对应向量的第<span class="math inline">\(j\)</span>个分量。</p>
<p>其中W可以由<span class="math inline">\(\phi(X)^T\phi(X)\)</span>的特征矩阵求出，但在计算<span class="math inline">\(z\)</span>时，由于每个分量需要求和<span class="math inline">\(\sum\limits_{k=1}^m\alpha_{k,j}\)</span>，计算量非常大。</p>
<h2 id="流形学习">流形学习</h2>
<p>流形是指在局部与欧式空间统配的空间，如果数据是嵌入在高维空间中的低维流形，则可以利用其局部与欧式空间同胚的性质，使用局部的欧式距离来计算数据样本之间的距离。</p>
<h3 id="等度量映射isometric-mappingisomap">等度量映射（Isometric
Mapping，Isomap）</h3>
<p>将低维嵌入流形上两点的距离定义为“测地线”距离，即两个样本沿着流形的最短距离，测地线距离的计算可以将邻近点之间进行连接，然后转换计算近邻连接图上两点之间的最短路径问题（Dijkstra算法或者Floyd算法）。得到距离矩阵之后，可以使用多维缩放算法（MDS）来进行降维。</p>
<p>邻近图的构建一般有两种做法，一个是指定最近的<span class="math inline">\(k\)</span>个点作为邻近点，这样得到的邻近图称为<span class="math inline">\(k\)</span>邻近图，另一个是指定距离阈值<span class="math inline">\(\epsilon\)</span>，小于<span class="math inline">\(\epsilon\)</span>的点被认为是邻近点，这样得到的邻近图称为<span class="math inline">\(\epsilon\)</span>邻近图，两种方式各有优劣。</p>
<h3 id="局部线性嵌入locally-linear-embeddinglle">局部线性嵌入（Locally
Linear Embedding，LLE）</h3>
<p>Isomap试图保持邻近样本之间的距离，而LLE试图保持邻域内样本的线性关系，假定样本点<span class="math inline">\(x_i\)</span>可以通过<span class="math inline">\(x_j,x_k,x_l\)</span>线性组合而得到，即<span class="math inline">\(x_i = w_{ij}x_j+w_{ik}x_k+w_{il}x_l\)</span></p>
<p>首先对于每个样本<span class="math inline">\(x_i\)</span>，找到其邻近下标集合<span class="math inline">\(Q_i\)</span>，然后计算<span class="math inline">\(Q_i\)</span>对<span class="math inline">\(x_i\)</span>的线性重构系数： <span class="math display">\[
\begin{aligned}
    \min\limits_{w_1, w_2, ..., w_m}\ &amp;\sum\limits_{i=1}^m ||x_i -
\sum\limits_{j\in Q_i}w_{ij} x_j||_2^2\\
    s.t.\ &amp;\sum\limits_{j\in Q_i} w_{ij} = 1
\end{aligned}
\]</span></p>
<p>求得<span class="math inline">\(w_{ij}\)</span>之后，<span class="math inline">\(x_i\)</span>对应的低维空间坐标<span class="math inline">\(z_i = \min\limits_{z_1, z_2, ..., z_m}
\sum\limits_{i=1}^m ||z_i - \sum\limits_{j\in Q_i} w_{ij}
z_j||_2^2\)</span>。</p>
<p>令<span class="math inline">\(Z = \begin{bmatrix}z_1 &amp; z_2 &amp;
\cdots &amp; z_m\end{bmatrix} \in R^{d&#39; \times m}, W_{ij} =
w_{ij}\)</span></p>
<p>则确定<span class="math inline">\(W\)</span>之后，<span class="math inline">\(Z\)</span>可以通过： <span class="math display">\[
\begin{aligned}
    \min\limits_Z\ &amp;tr(Z(\mathbf{I} - W)^T(\mathbf{I} - W)Z^T)\\
    s.t.\ &amp;ZZ^T = \mathbf{I}
\end{aligned}
\]</span> 来求得，即对<span class="math inline">\((\mathbf{I} -
W)^T(\mathbf{I} - W)\)</span>进行特征分解，取最小的<span class="math inline">\(d&#39;\)</span>个特征值对应的特征向量构成<span class="math inline">\(Z^T\)</span></p>
<h2 id="度量学习">度量学习</h2>
<p>对高维数据的降维主要是希望找到一个合适的低维空间使得此空间中学习能比原始空间性能更好，度量学习的思路是尝试学习出一个距离度量。</p>
<p>对于两个<span class="math inline">\(d\)</span>维样本<span class="math inline">\(x_i\)</span>，<span class="math inline">\(x_j\)</span>，其欧氏距离的平方<span class="math inline">\(||x_i - x_j||^2_2 = dist_{ij,1} + dist_{ij,2} +
\cdots + dist_{ij,d}\)</span>，其中<span class="math inline">\(dist_{ij,k}\)</span>表示在第<span class="math inline">\(k\)</span>维上的距离。</p>
<p>如果假定不同属性的重要性不同，则可以引入权重<span class="math inline">\(w\)</span>，<span class="math inline">\(||x_i -
x_j||^2_2 = w_1 dist_{ij,1} + w_2 dist_{ij,2} + \cdots + w_d dist_{ij,d}
= (x_i - x_j)^T W (x_i - x_j)\)</span>，其中<span class="math inline">\(W = diag(w), w_i \ge 0\)</span></p>
<p>如果令<span class="math inline">\(W\)</span>不再是一个对角矩阵，而是让其等于一个半正定对称矩阵<span class="math inline">\(M\)</span>，则可以定义马氏距离<span class="math inline">\(dist_{mah}^2(x_i, x_j) = (x_i - x_j)^TM(x_i - x_j)
= ||x_i - x_j||^2_M\)</span>，则可以对这个<span class="math inline">\(M\)</span>进行学习，得到满足要求的距离表达。</p>
<p>在近邻成分分析（Neighbourhood Component Analysis,
NCA）中可以用对<span class="math inline">\(M\)</span>进行训练，提高其分类正确率。</p>
<p>又例如根据一些领域知识已知某些样本相似（必连约束集合），另外一些样本不相似（勿连约束及合），则可以对<span class="math inline">\(M\)</span>进行训练，使得必连约束集合中的样本距离尽可能小，而勿连约束集合中的样本的尽可能大。</p>
<p>不管以任何方式训练得到的<span class="math inline">\(M\)</span>，都可以对M进行特征值分解，然后去掉一部分特征向量，得到降维矩阵，用于数据的降维。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9D%82%E9%A1%B9/" rel="tag"># 杂项</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
              <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" rel="tag"># 读书笔记</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/01/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%8AExpectation-Maximization-Attention-Networks-for-Semantic-Segmentation%E3%80%8B/" rel="prev" title="论文阅读《Expectation-Maximization Attention Networks for Semantic Segmentation》">
      <i class="fa fa-chevron-left"></i> 论文阅读《Expectation-Maximization Attention Networks for Semantic Segmentation》
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/04/23/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E9%9D%9E%E6%A0%87%E9%87%8F%E6%B1%82%E5%AF%BC%E6%9C%AF/" rel="next" title="非标量求导术">
      非标量求导术 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.</span> <span class="nav-text">线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%E6%9C%80%E5%B0%8F%E5%8C%96%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%B1%82%E8%A7%A3%E7%9A%84%E6%96%B9%E6%B3%95%E7%A7%B0%E4%B8%BA%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95"><span class="nav-number">1.1.</span> <span class="nav-text">基于均方误差最小化来进行求解的方法称为最小二乘法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E6%9D%A5%E4%BC%98%E5%8C%96%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.2.</span> <span class="nav-text">用最小二乘法来优化线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%9B%9E%E5%BD%92%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-logistic-regression"><span class="nav-number">1.3.</span> <span class="nav-text">对数几率回归（逻辑回归，
Logistic Regression）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90"><span class="nav-number">1.4.</span> <span class="nav-text">线性判别分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="nav-number">1.5.</span> <span class="nav-text">多分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%B1%BB%E5%88%AB%E4%B8%8D%E5%B9%B3%E8%A1%A1"><span class="nav-number">1.6.</span> <span class="nav-text">类别不平衡</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">2.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5"><span class="nav-number">2.1.</span> <span class="nav-text">信息熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E7%86%B5%E5%A2%9E%E7%9B%8A"><span class="nav-number">2.2.</span> <span class="nav-text">信息熵增益</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A2%9E%E7%9B%8A%E7%8E%87"><span class="nav-number">2.3.</span> <span class="nav-text">增益率</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0"><span class="nav-number">2.4.</span> <span class="nav-text">基尼指数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E5%89%AA%E6%9E%9D"><span class="nav-number">3.</span> <span class="nav-text">决策树的剪枝</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D"><span class="nav-number">3.1.</span> <span class="nav-text">预剪枝</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D"><span class="nav-number">3.2.</span> <span class="nav-text">后剪枝</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8C%85%E5%90%AB%E8%BF%9E%E7%BB%AD%E5%80%BC%E7%9A%84%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">4.</span> <span class="nav-text">包含连续值的决策树</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%B1%9E%E6%80%A7%E7%BC%BA%E5%A4%B1%E7%9A%84%E5%A4%84%E7%90%86"><span class="nav-number">5.</span> <span class="nav-text">属性缺失的处理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">5.1.</span> <span class="nav-text">多变量决策树</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">6.</span> <span class="nav-text">神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">6.1.</span> <span class="nav-text">感知机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bp%E7%AE%97%E6%B3%95"><span class="nav-number">6.2.</span> <span class="nav-text">BP算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rbfradial-basis-function%E5%BE%84%E5%90%91%E5%9F%BA%E5%87%BD%E6%95%B0%E7%BD%91%E7%BB%9C"><span class="nav-number">6.3.</span> <span class="nav-text">RBF（Radial Basis
Function，径向基函数）网络</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#svm"><span class="nav-number">7.</span> <span class="nav-text">SVM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">8.</span> <span class="nav-text">贝叶斯分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%A1%E4%BB%B6%E9%A3%8E%E9%99%A9"><span class="nav-number">8.1.</span> <span class="nav-text">条件风险</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E4%B8%8E%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B"><span class="nav-number">8.2.</span> <span class="nav-text">生成式与判别式模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="nav-number">8.3.</span> <span class="nav-text">极大似然估计</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">8.4.</span> <span class="nav-text">朴素贝叶斯分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%8A%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">8.5.</span> <span class="nav-text">半朴素贝叶斯分类器</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="nav-number">9.</span> <span class="nav-text">集成学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#boosting"><span class="nav-number">9.1.</span> <span class="nav-text">Boosting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging"><span class="nav-number">9.2.</span> <span class="nav-text">Bagging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bagging%E5%92%8Cboosting%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">9.3.</span> <span class="nav-text">Bagging和Boosting的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97random-forestrf"><span class="nav-number">9.4.</span> <span class="nav-text">随机森林（Random Forest，RF）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%BB%93%E5%90%88%E7%AD%96%E7%95%A5"><span class="nav-number">9.5.</span> <span class="nav-text">集成学习的结合策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E6%A0%B7%E6%80%A7"><span class="nav-number">9.6.</span> <span class="nav-text">多样性</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%81%9A%E7%B1%BB"><span class="nav-number">10.</span> <span class="nav-text">聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%9F%E5%9E%8B%E8%81%9A%E7%B1%BB"><span class="nav-number">10.1.</span> <span class="nav-text">原型聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#k%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95"><span class="nav-number">10.1.1.</span> <span class="nav-text">k均值算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E5%90%91%E9%87%8F%E9%87%8F%E5%8C%96learning-vector-quantizationlvq"><span class="nav-number">10.1.2.</span> <span class="nav-text">学习向量量化（Learning
Vector Quantization，LVQ）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BBmixture-of-gaussian"><span class="nav-number">10.1.3.</span> <span class="nav-text">高斯混合聚类（Mixture-of-Gaussian）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB"><span class="nav-number">10.2.</span> <span class="nav-text">密度聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dbscan"><span class="nav-number">10.2.1.</span> <span class="nav-text">DBSCAN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB"><span class="nav-number">10.3.</span> <span class="nav-text">层次聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#agnes"><span class="nav-number">10.3.1.</span> <span class="nav-text">AGNES</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="nav-number">11.</span> <span class="nav-text">降维与度量学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#k%E8%BF%91%E9%82%BBk-nearest-neighbor%E5%AD%A6%E4%B9%A0"><span class="nav-number">11.1.</span> <span class="nav-text">k近邻（k-Nearest
Neighbor）学习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%8E%E7%BB%B4%E5%B5%8C%E5%85%A5"><span class="nav-number">11.2.</span> <span class="nav-text">低维嵌入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%9A%E7%BB%B4%E7%BC%A9%E6%94%BEmultiple-dimensional-scalingmds"><span class="nav-number">11.2.1.</span> <span class="nav-text">多维缩放（Multiple
Dimensional Scaling，MDS）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90pca"><span class="nav-number">11.3.</span> <span class="nav-text">主成分分析（PCA）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A0%B8%E5%8C%96%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4"><span class="nav-number">11.4.</span> <span class="nav-text">核化线性降维</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90kpca"><span class="nav-number">11.4.1.</span> <span class="nav-text">核主成分分析（KPCA）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0"><span class="nav-number">11.5.</span> <span class="nav-text">流形学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AD%89%E5%BA%A6%E9%87%8F%E6%98%A0%E5%B0%84isometric-mappingisomap"><span class="nav-number">11.5.1.</span> <span class="nav-text">等度量映射（Isometric
Mapping，Isomap）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B1%80%E9%83%A8%E7%BA%BF%E6%80%A7%E5%B5%8C%E5%85%A5locally-linear-embeddinglle"><span class="nav-number">11.5.2.</span> <span class="nav-text">局部线性嵌入（Locally
Linear Embedding，LLE）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="nav-number">11.6.</span> <span class="nav-text">度量学习</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Tennen</p>
  <div class="site-description" itemprop="description">record something</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">47</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tennen</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
